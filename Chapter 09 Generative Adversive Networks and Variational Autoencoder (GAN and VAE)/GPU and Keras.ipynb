{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NooLwzmEvaoW"
      },
      "source": [
        "# **GPU with Keras**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFOL2gPGvaoX"
      },
      "source": [
        "You may have heard of GPUs (Graphics Processing Unit) and CPUs (Central Processing Unit), but what is the difference? GPUs have been commonly seen used by gamers for better visual rendering, but nowadays its applications extend way beyond improving videogame experience. With respect to deep learning, GPUs are extremely helpful by speeding up certain computations. The difference is evident especially for models that train on large datasets, in which the researcher can take advantage of parallel computing to run operations simultaneously and save time. In this lab, you will learn about how to utilize GPU for `tensorflow`, specifically `keras`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUTWjcoCvaoX"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/L1/img_GPU.jpeg\" width=\"600\" alt=\"computer components\">\n",
        "<center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niedtNX7vaoY"
      },
      "source": [
        "**_Note_**: Skills Network Labs currently doesn't have any GPUs available. In order to test the difference between CPU and GPU, please run this lab on a local machine or environment that has GPUs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6vxYaASvaoa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        " - Set environment to CPU/GPU\n",
        " - Control usage of CPU/GPU in parts of the code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT3E16Exvaob"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXTV_iZvaoe"
      },
      "source": [
        "### Importing Required Libraries\n",
        "\n",
        "_We recommend you import all required libraries in one place, as follows:_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "loZhGK_Lvaof"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "# Import the keras library\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.python.client import device_lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDdAGDhDvaof"
      },
      "source": [
        "## Benefits of Using GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rde0_WIqvaof"
      },
      "source": [
        "GPU excels in parallel computing in comparison to CPU. This technique is especially useful for deep learning algorithms, such as building a Convolutional Neural Network (CNN), or as a matter of fact, any neural network. An example of a parallel processing task is performing convolution on an input layer, in which the kernel is multiplied with the input layer matrix, one local region at a time.\n",
        "\n",
        "The runtime difference is especially noticeable when you train a CNN with multiple epochs - tasks where a lot of matrix operations are involved!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIVCEqFtvaog"
      },
      "source": [
        "## Using CPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fefpzhnTvaog"
      },
      "source": [
        "By default, `tensorflow` searches for available GPU to use. There are two ways to force your machine to ignore all GPUs and run code with CPU instead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHp1lA6Dvaog"
      },
      "source": [
        "If you want the entire code/notebook to run on CPU, you can specify the environment _**before**_ importing tensorflow/keras. If you decide to switch back, you can restart the kernel and run import as usual without the line below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jJBgnsq8vaog"
      },
      "outputs": [],
      "source": [
        "# Specify the environment variable value\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onekp4Q4vaoh"
      },
      "source": [
        "If the environment variable `CUDA_VISIBLE_DEVICES` value takes the values 0/1 (or other positive values), the machine is using GPU to run the code. By setting it to -1, it specifies the algorithm to be run with CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzem5O9ivaoh",
        "outputId": "66ebc4ee-6f47-4b54-9aca-e221acbb83d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1\n"
          ]
        }
      ],
      "source": [
        "# Check that CPU is used\n",
        "print(os.environ['CUDA_VISIBLE_DEVICES'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD2GaX4Kvaoh"
      },
      "source": [
        "If instead you want to use CPU for portions of the code in a notebook, consider the following approach. Here, you specify what to run with `/CPU:0` using a `with` statement. Using `%%timeit` with `-n1 -r1` will time the process for one pass of the cell. As an example, we'll be training the following CNN on a **DATASET**. Feel free to change the code within the statement to test CPU performance! \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QagBjlIPvaoh",
        "outputId": "588fc148-611e-46ee-ccea-8f572c0ba6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Import data\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kE_axCZbvaoi"
      },
      "outputs": [],
      "source": [
        "# Reshape the data\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
        "\n",
        "y_train = y_train.reshape((y_train.shape[0],1))\n",
        "y_test = y_test.reshape((y_test.shape[0],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lPAcFi5vaoi",
        "outputId": "da46822a-c468-4e72-8d9c-529e744dff6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 38s 19ms/step - loss: 1.1349 - accuracy: 0.9190 - val_loss: 0.2202 - val_accuracy: 0.9492\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1435 - accuracy: 0.9641 - val_loss: 0.1683 - val_accuracy: 0.9615\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1022 - accuracy: 0.9713 - val_loss: 0.1343 - val_accuracy: 0.9677\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0948 - accuracy: 0.9740 - val_loss: 0.1365 - val_accuracy: 0.9666\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0736 - accuracy: 0.9801 - val_loss: 0.1978 - val_accuracy: 0.9586\n",
            "2min 50s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n1 -r1\n",
        "# Building the CNN model and fitting on train data\n",
        "with tf.device('/CPU:0'):\n",
        "    model_cpu = Sequential()\n",
        "    model_cpu.add(Conv2D(input_shape = (28, 28, 1),\n",
        "                     filters=5, \n",
        "                     padding='Same',\n",
        "                     kernel_size=(3,3)\n",
        "                     ))\n",
        "    model_cpu.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model_cpu.add(Flatten())\n",
        "    model_cpu.add(Dense(256, activation='relu'))\n",
        "    model_cpu.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model_cpu.compile(optimizer='adam', \n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    model_cpu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klCYSUPxvaoj"
      },
      "source": [
        "## Using GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5RlQ3vYvaoj"
      },
      "source": [
        "As mentioned above, `tensorflow` automatically searches for GPUs to run on. Let's take a closer look at how we can have more control over that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvSIm6zWvaoj"
      },
      "source": [
        "### Check Availability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXE6qlvuvaoj"
      },
      "source": [
        "First, you can check the number of GPUs available on the machine. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCc83OD7vaok",
        "outputId": "e6e654e0-31fb-47a3-ac87-4de228a28a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6kmQypvvaok"
      },
      "source": [
        "If you're running this notebook in Skills Network Lab, you can see that it doesn't have any GPUs available for use. However, if your local machine does have GPU(s), you can try the following code to play with what you want to run on GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygmmoi49vaok"
      },
      "source": [
        "### Choosing Specific GPUs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxN2aXcivaol"
      },
      "source": [
        "In order to specify a particular GPU to run on, we have to first check what units there are in the environment. The following lists out the information of each device, including the device name, type, memory limit, and so on. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61oYeiA_vaol",
        "outputId": "a783a3e6-4357-4daa-db4c-b8633e89172a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8881978198773156056\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDHi22OLvaol"
      },
      "source": [
        "To specify using a specific GPU, again use `tf.device()` with the `name` as input, just like we did for the CPU case. In the `with` statement, proceed with writing code as usual. Here, we are specifying `tensorflow` to be run on GPU ennumerated #2. We also use `%%timeit` here so you can compare the time that GPU took to run in comparison with CPU!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_56PjziZvaol",
        "outputId": "f0eeebc8-415a-4088-b48d-9a5c4e3168e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 1.0976 - accuracy: 0.9198 - val_loss: 0.2006 - val_accuracy: 0.9488\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.1469 - accuracy: 0.9627 - val_loss: 0.1607 - val_accuracy: 0.9606\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1050 - accuracy: 0.9714 - val_loss: 0.1385 - val_accuracy: 0.9662\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0946 - accuracy: 0.9746 - val_loss: 0.1225 - val_accuracy: 0.9660\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0801 - accuracy: 0.9785 - val_loss: 0.1189 - val_accuracy: 0.9720\n",
            "3min 23s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n1 -r1\n",
        "with tf.device('/device:GPU:2'):\n",
        "    model_gpu = Sequential()\n",
        "    model_gpu.add(Conv2D(input_shape = (28, 28, 1),\n",
        "                     filters=5, \n",
        "                     padding='Same',\n",
        "                     kernel_size=(3,3)\n",
        "                     ))\n",
        "    model_gpu.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model_gpu.add(Flatten())\n",
        "    model_gpu.add(Dense(256, activation='relu'))\n",
        "    model_gpu.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model_gpu.compile(optimizer='adam', \n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    model_gpu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "0QhespxOvaov"
      },
      "source": [
        "## Using CPU and GPU jointly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RIGwbgGvaov"
      },
      "source": [
        "What if we want to use _both_ CPU and GPU for different parts of the same python script? Turns out we can do that too! Simply take advantage of the `tf.device()` function again to specify which unit the code fragment should be run on. Below, we show an example of how to run the same matrix operation on multiple GPUs and add up the tensors on CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6nIAs0VAvaov"
      },
      "outputs": [],
      "source": [
        "# Enable tensor allocations or operations to be printed\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "# Get list of all logical GPUs\n",
        "gpus = tf.config.list_logical_devices('GPU')\n",
        "\n",
        "# Check if there are GPUs on this computer\n",
        "if gpus:\n",
        "  # Run matrix computation on multiple GPUs\n",
        "    c = []\n",
        "    for gpu in gpus:\n",
        "        with tf.device(gpu.name):\n",
        "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) \n",
        "            c.append(tf.matmul(a, b))\n",
        "\n",
        "    # Run on CPU \n",
        "    with tf.device('/CPU:0'):\n",
        "        matmul_sum = tf.add_n(c)\n",
        "\n",
        "    print(matmul_sum)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}